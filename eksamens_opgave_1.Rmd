---
title: "Eksamens opgave 1 - OLS og Hetroskedasticitet"
author: "Lars Boerty Nielsen - 20134303"
date: "25 sep 2017"
output:
  html_document: default
---
Betragt f?lgende model for bankansattes l?n:
$$\log(salar y) = \beta_0+\beta_1educ +\beta_2 log(salbeg in)+\beta_3male +\beta_4minor i t y +u$$
  
 - $salary$ er ?rsl?n (i 1000 US dollars)
 - $educ$ er uddannelse m?lt i antal ?r
 - $salbegin$ er startl?nnen (i 1000 US dollars) for personens f?rste stilling i samme bank
 - $male$ er en dummy-variabel for k?n
 - $minority$ er en dummy-variabel der angiver om man tilh?rer en minoritet

Datas?ttet data1 (wage), som er tilg?ngelig p?Moodle, indeholder disse variable m?lt for 474 bankansatte.

```{r, echo=FALSE,warning=FALSE, message=FALSE}
library(tidyverse)
library(texreg)
library(lmtest)
library(sandwich)
library(foreign)
library(car)
library(ggplot2)

wage <- read_csv("educ_salary_data.csv")
```

##Opgave 1: Estimer modellen vha. OLS. Kommenter p? outputtet og fortolk resultaterne.
```{r}
#MLR - Multiple lin?r regression
reg1 <-lm(lsalary ~ educ + lsalbegin + male + minority, data = wage)
summary(reg1)
```


```{r}
ggplot(wage, aes(lsalary, educ)) + 
  geom_point() + 
  geom_smooth(method='lm')
```





Det kan her ses at modellen som er log - level (udover lsalbegin) 
  
- alle er signifikante p? et 5% niveau
- 1 ?rs uddannelse give 2% h?jere l?n
- at v?re mand giver 4% h?jere l?n
- at v?re minioritet giver ca. 4% lavere l?n
- log-log 1% h?jere startl?n giver 0.8% h?jere l?n

ud fra $R^2$ "goodness of fit" er 80% af l?nnens variation forklaret med parametrene (hvis det kun er uddannelse der er den uafh?ngige parameter s? kan 48% af l?nnen forklares af dette) 

$$SST = SSE + SSR$$
$$R^2 \equiv SSE/SST = 1-SSR/SST$$
$$SSE=\sum_{i=1}^{n}(\hat{y_i}-\bar{y})^2 
\qquad SST=\sum_{i=1}^{n}({y_i}-\bar{y})^2 
\qquad SSR=\sum_{i=1}^{n}(\hat{u_i})^2$$

Al variation i en variabel y kan, med en uafh?ngig variabel, x, forklares med to ting: den del af variationen der forklares af x (SSE) og den del der ikke forklares af x (SSR). Vi har $R^2$ som er et m?l for hvor meget vi kan forklare.

OLS m?l er at minimere SSR 

Vi kan ogs? se at uddannelse og start l?n er meget signigikante, mens k?n og minioritet ikke er s?rligt, men kan dog afvise at der ikke er nogen effekt p? 5% signifikansniveau.

(En detalje er at modellen m?ske ikke fanger alt diskriminationen. Vi skal forst? minority estimatet som n?r de andre faktorere holdes fast, dette giver selvf?lgelig mening med uddannelse, at sammenligne minoritet og ikke minoritet p? samme uddannelsesniveau, men vi holder ogs? startl?nnen fast og kan derfor ikke se om en minoritet til at starte med betales mindre. Dette bekr?ftes hvis vi fjerner lsalbegin fra modellen s? f?r vi en v?sentligt h?jere diskrimination af minioriteter.)

\pagebreak

##Opgave 2: Udf?r grafisk modelkontrol.
```{r, echo=FALSE}
par(mfrow = c(2,2))
plot(reg1)
#correlation between salary and education
#plot(lsalary~educ, pch=16, cex=1.3, main = "L?n ~ uddannelse", xlab = "Uddannelse i ?r", ylab = "Log salary")
#abline(lm(lsalary~educ))

#correlation between salary and sex
#plot(lsalary~male, pch=16, cex=1.3, main = "L?n ~ k?n", xlab = "Male 1 = ja", ylab = "Log salary")
#abline(lm(lsalary~male))

#correlation between salary and minority
#plot(lsalary~minority, pch=16, cex=1.3, main = "L?n ~ minioritet", xlab = "Minoritet 1 = ja", ylab = "Log salary")
#abline(lm(lsalary~minority))

#correlation between salary and starting salary
#plot(lsalary~lsalbegin, pch=16, cex=1.3, main = "L?n ~ startl?n", xlab = "Log startl?n", ylab = "Log salary")
#abline(lm(lsalary~lsalbegin))
```

1. graf kan man se at residualerne er lin?re og normaltfordelte

2. graf bekr?ftes det at residualerne er normaltfordelte, med et par outliers, dette er vigtigt i forhold til hypotesetest og se()

3. graf ser ogs? fin ud da der ikke er nogle st?rre udsving i de standardiserede residualer over de fittede v?rdier 

4. graf viser at der ikke er store outliers

P? de 4 sidste grafer er der individuelle regressioner og de bekrafter forholdet i den oprindelige korrelation mellem de 3 positive og en negativ mellem l?n og minioritet

\pagebreak

##Opgave 3: Test for heteroskedasticitet vha. Breusch-Pagan-testet og specialudgaven af White-testet
Hetroskedasticitet er n?r variansen ikke er konstant over $u$ Dette skal v?re opfyldt i for at kunne hypoteseteste og udregne standard fejl. Her ses kravet for hetroskedasticitet, $H_1$:
$$H_0=Var(u|x)=\sigma^2 
\qquad 
H_1=Var(u|x)\neq\sigma^2 
$$

BP-testen og specialudgaven af White testen kan teste for hetroskedasticitet ($h_0$ er i begge tilf?lde er **homoskedasticitet**). Begge test tager udgangspunkt i sqres ($\hat{u}^2$)  
* hvor BP-testen laver en ny regression ift. de uafh?ngige variabler (og benytter en lagrange multiplier test LM)
* en specielle White g?r det ift. fitted values og kvardrede fitted values.

$$u^2=\delta_0+\delta_1x_1+\delta_2x_2+.+\delta_kx_k+v$$
$$LM=R^2_{\hat{u}} *n$$

F?rst gennemg?r vi Breusch-Pagan-testen
```{r,warning=FALSE}
#Manuel BP-test - Jeg tager "squared residuals" (u^2)
sqres<-(reg1$residuals)^2

#Jeg k?rer nu en ny regression med sqres som afh?ngig variabel (her kan BP-testen med F-statistikken afl?ses direkte)
reg2<-lm(sqres ~ educ + lsalbegin + male + minority, data = wage)
summary(reg2)

#Nu laver vi BP-testen med LM-statistikken hvor vi bruger r^2 v?rdien fra vores regression
r2 <- 0.02923

1 - pchisq((r2*length(wage$salary)), 4)

#Breusch-Pagan-test med R (bruger som standard LM-statistikken)
bptest(reg1)
```
- **BP-test med F-statistikken: P=0.00747**  
For effekten p? de kvardrede residualer kan vi afvise at den er homoskedastisk, vores $H_0$, den er alts? hetroskedastisk. Det vil alts? sige at de uafh?ngige variabler kan forklare ?ndringer i residualerne

- **BP-test med LM-statistikken P=0.00776**  
LM-statistikken (som benytter en chi^2 fordeling) bekr?fter hvad vi ved fra F-statistikken (kan afvises p? 5% signifikansniveau)

Modellen lider alts? derfor af hetroskedasicitet. Det betyder at variansen af residualerne ikke er konstant over regressionen. Det bryder med MLR5 antagelsen om at 

$$var(u\mid x_1,...,x_k)=\sigma^2$$
Dette betyder at vores model bryder med BLUE (Best Linary Unbiased Model)

Nu unders?ger vi med White testen (s.253)

```{r}
#Vi skal bruge fittedvalues^2 for at bruge specialudgaven af White testen
sqfitted <- (reg1$fitted.values)^2
#Nu k?rer vi en ny regression med disse og fitted values sammen med residualerne^2 
reg3<-lm(sqres ~ reg1$fitted.values + sqfitted)
summary(reg3)
#Vi afl?ser den nye r^2 v?rdi som bruges ved F og LM testen (F-testen er kan allerede afl?ses i summary)
r2.1<-0.02295

1 - pchisq((r2.1 * length(wage$salary)),2)
```


- **White-test med F-statistikken: P=0.00422**
- **White-test med LM-statistikken: P=0.00434**

Her kan vi igen afvise homoskedacitet ved 5% signifikansniveau

\pagebreak

##Opgave 4: Beregn robuste standardfejl for modellen og sammenlign med resultaterne i sp?rgsm?l 1.
Vi kan udregne disse med R: disse er anderledes fordi at modellen udviser hetroskedasitet. Udregningen af disse er en nyere metode og matematisk uden for pensum, men den korregerer for ukendt hetroskedasticitet. Den kan g?re dette p? flere m?der som afh?nger af hvordan data ser ud. Jeg bruger HC0

$$
\text{SLR: robuste varians:}  \qquad
var(\beta_1)=\frac{\sum_{i=1}^{n}(x_i-\bar{x})^2\sigma^2_{i}}{SSR_x^2}
\qquad \rightarrow \qquad
\sigma^2_i \rightarrow \hat{u}^2_i 
\qquad \rightarrow \qquad
var(\hat\beta_1)=\frac{\sum_{i=1}^{n}(x_i-\bar{x})^2\hat u^2_{i}}{SSR_x^2}$$



```{r}
reg1.r <- coeftest(reg1, vcov = vcovHC(reg1, type = "HC0"))
screenreg(list(REG = reg1, ROBUST_REG = reg1.r), digits = 4)
```

Det ses at estimaterne er de samme, men standard fejlene er anderledes, nogle er st?rre andre er mindre. Dette passer med at vi ved at hetroskedasticitet ikke er et problem for at finde estimaterne, det er f?rst n?r vi skal lave f?lgeslutninger (hypotesetest, etc.)

\pagebreak

##Opgave 5: Test hypotesen $H_0 :\beta_2 = 1 \qquad\qquad H_1 : \beta_2 \neq 1$  
hypotesen er at log l?n er lig med 1, jeg skal se om den kan forkastes

Normalt udregnes t-v?rdien som 
$$t=\frac{\hat\beta_2-\beta)}{se(\hat\beta_2)}$$
Hvor den sande beta er = o, dette giver t-v?rdien = 22.808 (Vi ville klart forkaset at logl?n = 0, men dette er ikke sp?rgsm?let)
men vi g?r nu
$$t=\frac{\hat\beta_2-1)}{se(\hat\beta_2)}$$

```{r}
(0.8218-1)/0.036
qt(0.025,472)
2*pt(-4.95,472)
```
Dette giver t-v?rdien = -4.95 (dette kan vi allerede se ikke er s? godt, da n er stor, men vi kan finde en p-v?rdi)
$H_0$ kan afvises p? 5% signifikansniveau da vi afviser $H_0$ hvis $t<-1.96$ eller $t>1.96$
Jeg kan ogs? udregne p-v?rdien associeret med denne t-v?rdi, dette giver p=0.00000103

Jeg forkaster at logl?n er lig med 1
\pagebreak

##Opgave 6: Test hypotesen $H_0:\beta_3 = \beta_4 = 0$
Der er 2 forskellige m?der at forst? opgaven:


1) At begge skal v?re lig 0, dette kan testes med en restricted model og en F-test
$$F \equiv \frac{(SSR_r-SSR_{ur})/q} {SSR_{ur}/(n-k-1)}$$
q = Nummerator degrees of freedom, alts? hvor mange f?rre parametre den restricted model har (2),  
k = Antal af parametre i modellen, den unrestricted (4)
Dette kan ogs? udregnes med $R^2$ v?rdierne hvis man ikke har $SSR$

```{r}
linearHypothesis(reg1, c("male = 0","minority = 0"))

#Unrestricted model og unrestricted
ols.reg.unrestricted <- lm(lsalary ~ educ + lsalbegin + male + minority, data = wage)
ols.reg.restricted   <- lm(lsalary ~ educ + lsalbegin, data = wage)

#Beregn q (antallet af variabler fjernet fra modellen)
q <- ols.reg.restricted$df.residual - ols.reg.unrestricted$df.residual

#Opn? r2
unrestricted.r2 <- summary(ols.reg.unrestricted)$r.squared
restricted.r2   <- summary(ols.reg.restricted)$r.squared

#Beregning af F-test
F.val <- ((restricted.r2 - unrestricted.r2)/q)/((unrestricted.r2 - 1)/(471 - 2))
1 - pf(F.val, 2, 467)
```
Vi kan se at vi med en F-test kan afvise at de to parametre begge er 0, med en p-v?rdi p? 0.015

2) Den anden m?de at forst? opgaven: at estimatet p? male og minority er lig med hinanden. I dette tilf?lde finder vi t-v?rdien med f?lgende formel:
$$t=\frac{(\hat\beta_3-\hat\beta_4)}{se(\hat\beta_3-\hat\beta_4)}$$
desv?rre er 
$$se(\hat\beta_3-\hat\beta_4) \neq se(\hat\beta_3)-se(\hat\beta_4)$$
men vi kan g?re dette 
$$se(\hat\beta_3-\hat\beta_4) = (se(\hat\beta_3)^2-se(\hat\beta_4)^2 - cov(\hat\beta_3,\hat\beta_4))^{0.5}$$
Det er dog lidt besv?rligt at udregne det sidste, men derfor kan jeg g?re dette

F?rst laver vi en ny variabel som er lig med forskellen mellem de to estimater, vi vil gerne vide om denne kunne v?re = 0 og derved muligg?re at estimatet for l?np?virkningen af k?n og etcinitet er ens. 
$$\hat\beta_3-\hat\beta_4=\theta
\qquad \rightarrow \qquad 
\hat\beta_3=\theta+\hat\beta_4$$
Vi kan inds?tte dette i vores model, og omrokere
$$lsalary = \beta_0 + \beta_1educ + \beta_2lsalbegin + (\theta+\beta_4)male + \beta_4minority)$$
$$lsalary = \beta_0 + \beta_1educ + \beta_2lsalbegin + \theta male + \beta_4(minority+male)$$

Nu kan jeg k?re modellen igen og denne gang finde $se(\theta)$ for at finde konfidensintervallet for $\theta$ s? skal vi +/- med ca. 2*standard fejlen
```{r}
#Jeg definerer theta og min samlede variabel
theta <- summary(reg1)$coefficients[4,1] - summary(reg1)$coefficients[5,1]
wage$malenority <- wage$male + wage$minority

#Nu k?rer jeg regressionen med den samlede
reg3 <- lm(lsalary ~ educ + lsalbegin + male + malenority, data = wage)
summary(reg3)$coefficients

#Nu skal vi bruge standard fejlen p? theta (som nu er p? male)
setheta<-summary(reg3)$coefficients[4,"Std. Error"]
CI <- c((theta-(1.96*setheta)),(theta+(1.96*setheta)))

#Nu f?r vi 95% konfidensintervallet p? theta, som jo skal v?re 0
CI
```
vi kan alts? se at konfindens intervallet mellem $\beta_3$ og $\beta_4$ er: $[0.02925048 ; 0.151798]$
Dette inkluderer ikke vores $H_0$ og vi kan derfor afvise $H_0$ med et 95% konfidensinterval

Det betyder i realiteten at der er forskel p? k?nnet og om man er minioritet n?r det kommer til l?n. Vi kan b?de afvise at de skulle v?re 0, og at "bias" skulle v?re ens. (vi kan ogs? se p? P-v?rdien at vi kan afvise ned til et 0.01% signifikansniveau)



\pagebreak

##Opgave 7: Estimer modellen vha. FGLS og kommenter p? resultaterne
Vi har tidligere vist at modellen er homoskedastisk, og vi skal nu korrigerer for det (en anden metode end den robuste)

$$var(u\mid x) = \sigma^2exp(\delta_0+\delta_1x_1+\delta_2x_2+\delta_3x_3+\delta_4x_4)$$

$$log(u^2) = a_0+\delta_1x_1+\delta_2x_2+\delta_3x_3+\delta_4x_4)+e$$
med denne f?r vi de fitted values som vi inds?tter i formlen $\hat{h_i}=exp(\hat{g}_i)$ Dette giver os vores v?gt $w$

```{r}
lsqres <- log(sqres)
reg4 <- lm(lsqres ~ educ + lsalbegin + male + minority, data = wage)
w <- exp(fitted(reg4))
reg5 <- lm(lsalary ~ educ + lsalbegin + male + minority, weight=1/w, data = wage)

screenreg(list(REG = reg1, ROBUST = reg1.r, FGLS = reg5), digits = 4)
```

Vi kan se at FGLS modellen har mindsket standard fejlene (se) men kun en lille smule


\pagebreak

##Opgave 8: Har FGLS estimationen taget h?jde for al heteroskedasticiteten?
```{r}
bptest(reg1)
bptest(reg5)

```

```{r echo=FALSE}
plot(reg5, which=c(2))
```

Nej, der er stadig hetroskedasticitet